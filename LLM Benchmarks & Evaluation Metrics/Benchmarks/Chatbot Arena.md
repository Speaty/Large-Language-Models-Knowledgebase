[Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference](https://arxiv.org/abs/2403.04132)

A live, human preference benchmark addressing issues with static and ground truth methods of evaluating LLMs. Ground-truth-based benchmarks are not open ended, therefore do not reflect real world interactive flexibility. Static benchmarks can become contaminated over time. 

Pairwise comparison mechanism - users only need to compare two model responses and vote for the best instead of providing an absolute score


### Stats 
- 240,000 votes
- 90,000 users

### TODO:
- Look into BT coefficients
- 