Embeddings are the numerical vector representation of words or tokens. 

Embeddings are usually dense, low-dimensional vectors (usually consisting of real numbers) that represent high-dimensional objects, such as words or phrases. These vectors are generated in such a way that **similar words or concepts have vectors that are close to each other** in the embedding space, while dissimilar words are represented by vectors that are farther apart.

The process of converting words into these vectors is called **embedding**.

## Tokenisation


## Sparse Embeddings
