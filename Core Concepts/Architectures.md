# Architectures

## Transformer Models

## Attention Mechanism 
Attention allows models to focus on specific parts of the input sequence, enhancing their ability to capture dependencies and relationships in data.
- [Attention is All You Need](https://arxiv.org/abs/1706.03762) 
- [Attention Is Not All You Need Anymore](https://arxiv.org/abs/2308.07661)

## Selective State Models (SSMs)
- [Mamba Linear-Time Sequence Modelling with Selective State Spaces](https://arxiv.org/abs/2312.00752)  